apiVersion: veecode.backstage.io/v1alpha1
kind: Cluster
metadata:
  name: "cluster-playrole-vtg"
  environment:
    cluster_type: ec2
    domain: vkpr.platform.vee.codes
    ingress_type: kong
    certificate_account_email: platformss@vee.codes
    certificate_environment_type: production
    grafana_obs_api: https://grafana.ambima-cluster-demo.vkpr.platform.vee.codes
    public_ip: 44.195.222.91
  annotations:
    backstage.io/kubernetes-id: "cluster-playrole-vtg"
    github.com/project-slug: veecode-homolog/cluster-playrole-vtg
    backstage.io/techdocs-ref: dir:..
    github.com/workflows: terraform-deploy.yml,start-instance.yml,stop-instance.yml,kubeconfig.yml,terraform-destroy.yml
    cluster/instructions: "# Run the following commands to import the kubeconfig:\n  ssh -i ./cert.pem -o StrictHostKeyChecking=no $USERNAME@$44.195.222.91 \"mkdir -p .kube && k3d kubeconfig get k3s > ~/.kube/config\"\n  scp -i ./cert.pem $USERNAME@$44.195.222.91:~/.kube/config ~/.kube/config-cluster-playrole-vtg\n  yq -e 'del(.clusters[0].cluster.certificate-authority-data) | .clusters[0].cluster.insecure-skip-tls-verify=true | .clusters[].cluster.server |= sub(\"0.0.0.0\", \"44.195.222.91\")' -i ~/.kube/config-cluster-playrole-vtg\n  export KUBECONFIG=~/.kube/config-cluster-playrole-vtg\n  kubectl get pods -A\n"
    veecode/cluster-name: cluster-playrole-vtg
    kubernetes.io/secret-name: cluster-playrole-vtg-secret
    kubernetes.io/secret-namespace: veecode-homolog
    kubernetes.io/auth-provider: custom
    kubernetes.io/api-server: https://44.195.222.91:6550
    kubernetes.io/skip-tls-verify: "true"
    kubernetes.io/skip-metrics-lookup: "false"
spec:
  type: ec2
  lifecycle: experimental
  owner: "group:default/admin"
  environment: 'environment:default/platform_homolog_environment'
